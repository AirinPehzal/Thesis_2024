{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b735fe27",
   "metadata": {},
   "source": [
    "Loading the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f3bfaf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a219b90",
   "metadata": {},
   "source": [
    "Code for creating the ontology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6c520e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "'''\n",
    "Calculates the cosine simularity between words\n",
    "'''\n",
    "cosine_function = lambda a, b: np.inner(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))\n",
    "\n",
    "\n",
    "def conditional_prob(x, y):\n",
    "    '''\n",
    "    Calculates the conditional probability of having x in the document given y and vice versa\n",
    "    '''\n",
    "    x_given_y = np.count_nonzero(pd.Series(x * y)) / np.count_nonzero(y)\n",
    "    y_given_x = np.count_nonzero(pd.Series(x * y)) / np.count_nonzero(x)\n",
    "    return x_given_y, y_given_x\n",
    "\n",
    "\n",
    "def weighted_n(x, y):\n",
    "    '''\n",
    "    Calculates the weighted similarity between words\n",
    "    '''\n",
    "    return len(set(x) & set(y)) / ((len(x) + len(y) / 2))\n",
    "\n",
    "\n",
    "def L(x, y, docs_words, cooccured):\n",
    "    '''\n",
    "    Calculates the L metric for discovering hierarchical links \n",
    "    '''\n",
    "    P_x_y, P_y_x = conditional_prob(docs_words[x], docs_words[y])\n",
    "    c_x_y = cosine_function(docs_words[x], docs_words[y])\n",
    "    N_x_y = weighted_n(cooccured[x], cooccured[y])\n",
    "    return (P_y_x - P_x_y) * c_x_y * (N_x_y + 1)\n",
    "\n",
    "\n",
    "def find_ind_for_removal(known_edges, all_edges):\n",
    "    '''\n",
    "    Finds the index of the word for which we can potentially remove an edge from the graph\n",
    "    '''\n",
    "    for i in range(len(all_edges)):\n",
    "        if all_edges[i][0] == known_edges[0] and all_edges[i][1] == known_edges[1]:\n",
    "            return i\n",
    "\n",
    "\n",
    "def s(x, y, docs_words, G1):\n",
    "    '''\n",
    "    Calculates the s metric for discovering related/equivalent words\n",
    "    '''\n",
    "    cos_sim = cosine_function(docs_words[x], docs_words[y])\n",
    "    parents = nx.ancestors(G1, x) & nx.ancestors(G1, y)\n",
    "    permutated_parents = list(itertools.combinations(parents, 2))\n",
    "\n",
    "    av_sim = []\n",
    "    sup_sim = 0\n",
    "    for pair in permutated_parents:\n",
    "        av_sim.append(cosine_function(docs_words[pair[0]], docs_words[pair[1]]))\n",
    "\n",
    "    if av_sim != []:\n",
    "        sup_sim = sum(av_sim) / len(av_sim)\n",
    "\n",
    "    P_x_y, P_y_x = conditional_prob(docs_words[x], docs_words[y])\n",
    "    abs_dif = np.abs(P_x_y - P_y_x)\n",
    "\n",
    "    return cos_sim - 0.2 * sup_sim - 0.2 * abs_dif\n",
    "\n",
    "\n",
    "def distance(x, y, docs_words, G1):\n",
    "    '''\n",
    "    Calculates the distance between words as 1 / s(x, y)\n",
    "    If the distance is too small, returns a numbers obviously outside of range\n",
    "    '''\n",
    "    d = s(x, y, docs_words, G1)\n",
    "    if d != 0:\n",
    "        return 1 / d\n",
    "    else:\n",
    "        return 1000\n",
    "\n",
    "\n",
    "def single_linkage_distance(clust_a, clust_b, docs_words, G1):\n",
    "    '''\n",
    "    Calculates the single linkage distance for 2 clusters\n",
    "    '''\n",
    "    minimum_dist = 100\n",
    "    a = pd.concat([clust_a, clust_b])\n",
    "\n",
    "    all_links = list(itertools.combinations(list(a), 2))\n",
    "\n",
    "    for link in all_links:\n",
    "        d = distance(link[0], link[1], docs_words, G1)\n",
    "\n",
    "        if d < minimum_dist:\n",
    "            minimum_dist = d\n",
    "\n",
    "    return minimum_dist\n",
    "\n",
    "\n",
    "def create_ontology(united_columns, min_df=0.01, top_ngram=1, min_cooccur=10):\n",
    "    '''\n",
    "    Function for creating the ontology\n",
    "    '''\n",
    "    #Initializing the value which checks if the ontology is ready\n",
    "    check = -1\n",
    "\n",
    "    while check != 0:\n",
    "        \n",
    "        #Initializing TF-IDF with inputed parameters and using it on the input data\n",
    "        tfidf = TfidfVectorizer(ngram_range=(1, top_ngram), min_df=min_df)\n",
    "        tf_idf_united = tfidf.fit_transform(united_columns)\n",
    "        #Results of TF-IDF are saved into docs_words - dataframe which has words as columns and document indexes as rows\n",
    "        docs_words = pd.DataFrame(tf_idf_united.toarray(), columns=tfidf.get_feature_names_out())\n",
    "        \n",
    "        cooccured = {}\n",
    "        \n",
    "        #For every word in docs_words we create a list of cooccured words \n",
    "        #which happen in min_occur or more documents where the word is\n",
    "        for cur_word in enumerate(docs_words.columns):\n",
    "            current_list = []\n",
    "            for other_word in enumerate(docs_words.columns):\n",
    "                if cur_word[0] != other_word[0] and np.count_nonzero(\n",
    "                        pd.Series(docs_words.iloc[:, cur_word[0]] * docs_words.iloc[:, other_word[0]])) >= min_cooccur:\n",
    "                    current_list.append(other_word[1])\n",
    "            \n",
    "            cooccured[cur_word[1]] = current_list\n",
    "\n",
    "        work_nodes = set()\n",
    "        work_edges = []\n",
    "        \n",
    "        #For every word in docs_words and their cooccured words\n",
    "        #we calculates L metric\n",
    "        #if it is above the critical value, then we infer a hierarchical link\n",
    "        for word in docs_words.columns:\n",
    "            for other_word in cooccured[word]:\n",
    "                if L(word, other_word, docs_words, cooccured) > 0.2:\n",
    "                    work_nodes.add(word)\n",
    "                    work_nodes.add(other_word)\n",
    "                    work_edges.append(\n",
    "                        [other_word, word, {'weight': np.round(L(word, other_word, docs_words, cooccured), 4)}])\n",
    "\n",
    "        just_edges = {}\n",
    "        \n",
    "        #Getting a list of all edges in the ontology\n",
    "        for edge in work_edges:\n",
    "            if edge[0] not in just_edges:\n",
    "                just_edges[edge[0]] = [edge[1]]\n",
    "            else:\n",
    "                just_edges[edge[0]].append(edge[1])\n",
    "\n",
    "        edge_for_removal = []\n",
    "        \n",
    "        #Finding all edges which make a triangle\n",
    "        for cur_node in just_edges:\n",
    "            for other_node in just_edges[cur_node]:\n",
    "                if other_node in just_edges and set(just_edges[cur_node]) & set(just_edges[other_node]) != set():\n",
    "                    edge_for_removal.append(\n",
    "                        [cur_node, other_node, list(set(just_edges[cur_node]) & set(just_edges[other_node]))])\n",
    "        \n",
    "        #Removing the weakest edge (= with the least value of L metric)\n",
    "        for edges in edge_for_removal:\n",
    "            for member in edges[2]:\n",
    "                ind1 = find_ind_for_removal([edges[0], member], work_edges)\n",
    "                ind2 = find_ind_for_removal([edges[1], member], work_edges)\n",
    "\n",
    "                if ind1 is not None and ind2 is not None:\n",
    "                    if work_edges[ind1][2]['weight'] >= work_edges[ind2][2]['weight']:\n",
    "                        work_edges.pop(ind2)\n",
    "                    else:\n",
    "                        work_edges.pop(ind1)\n",
    "        \n",
    "        #Saving the ontology into a graph structure\n",
    "        G1 = nx.DiGraph()\n",
    "\n",
    "        G1.add_nodes_from(work_nodes)\n",
    "        G1.add_edges_from(work_edges)\n",
    "\n",
    "        rel_equiv = set()\n",
    "\n",
    "        visited = set()\n",
    "        \n",
    "        #Checking all words in the ontology for possible related/equivalent relationship\n",
    "        #If s metric for two words is above the critical value\n",
    "        #We assume that they are related/equivalent\n",
    "        for word in list(nx.nodes(G1)):\n",
    "            for other_word in list(nx.nodes(G1)):\n",
    "                if word != other_word and other_word not in visited:\n",
    "                    if s(word, other_word, docs_words, G1) >= 0.75:\n",
    "                        rel_equiv.add(word)\n",
    "                        rel_equiv.add(other_word)\n",
    "            visited.add(word)\n",
    "        \n",
    "        #Manually conctructed hierarchical clustering algoritm using single linkage = 1/s metric\n",
    "        clustering = pd.DataFrame(list(rel_equiv), columns=['word'])\n",
    "        clustering['cluster'] = list(clustering.index)\n",
    "\n",
    "        curr_clusters = clustering['cluster'].copy()\n",
    "        visited_clusts = set()\n",
    "\n",
    "        for clust in curr_clusters:\n",
    "            visited_clusts.add(clust)\n",
    "            curr_clust = clustering[clustering['cluster'] == clust]\n",
    "            other_clust = clustering[clustering['cluster'] != clust]\n",
    "\n",
    "            for other in other_clust['cluster']:\n",
    "                if other not in visited_clusts:\n",
    "                    another = other_clust[other_clust['cluster'] == other]\n",
    "\n",
    "                    if single_linkage_distance(curr_clust['word'], another['word'], docs_words, G1) <= (1 / 0.75):\n",
    "                        clustering.loc[clustering['cluster'] == other, ['cluster']] = clust\n",
    "        \n",
    "        #Printing the results of clustering to see what was found\n",
    "        print(clustering)\n",
    "        \n",
    "        #Checking if ontology is ready\n",
    "        #If yes, the algorithm stops and returns the final processed dataset\n",
    "        check = len(clustering['cluster']) - len(clustering['cluster'].unique())\n",
    "        \n",
    "        #If not\n",
    "        if check != 0:\n",
    "            \n",
    "            #For every cluster we create an aggravated word (separator between them is _)\n",
    "            for cluster in clustering['cluster'].unique():\n",
    "                curr = clustering[clustering['cluster'] == cluster]\n",
    "                in_work = curr.word.str.replace(' ', '_')\n",
    "                new_word_unique = set('_'.join(in_work).split('_'))\n",
    "                new_word = '_'.join(list(new_word_unique))\n",
    "                clustering.loc[clustering['cluster'] == cluster, ['replace']] = new_word\n",
    "\n",
    "            k = united_columns.copy()\n",
    "\n",
    "            new_united = []\n",
    "            \n",
    "            #Processing the original dataset to include the related/equivalent relationships\n",
    "            for row in k:\n",
    "                \n",
    "                #For every cluster we take the list of original words and the aggrevated word\n",
    "                #for this cluster\n",
    "                for cluster in clustering['cluster'].unique():\n",
    "                    words = clustering[clustering['cluster'] == cluster]\n",
    "                    words = words['word']\n",
    "                    replacement = clustering[clustering['cluster'] == cluster]\n",
    "                    replacement = replacement['replace'].iloc[0]\n",
    "                    \n",
    "                    #Remove all original words from the document\n",
    "                    count = 0\n",
    "                    for w in words:\n",
    "                        while row.find(w) != -1:\n",
    "                            count += 1\n",
    "                            row = row.replace(w, ' ')\n",
    "                            \n",
    "                    #If any words were removed, normalize the whitespaces\n",
    "                    while row.find('  ') != -1:\n",
    "                        row = row.replace('  ', ' ')\n",
    "                        \n",
    "                    #If any words from this cluster were removed, add the aggrevated word at the end of the document\n",
    "                    if count != 0:\n",
    "                        row = row + ' ' + replacement\n",
    "\n",
    "                new_united.append(row)\n",
    "            \n",
    "            #Update the dataset and start Klink algorithm again\n",
    "            united_columns = new_united\n",
    "    \n",
    "    #Return the ontology\n",
    "    return united_columns\n",
    "\n",
    "\n",
    "def visualise_ontology(united_columns, check_vis_list = False, min_df=0.01, top_ngram=1, min_cooccur=50):\n",
    "    '''\n",
    "    Function for visualising the ontology\n",
    "    '''\n",
    "    #Everything is the same as in create_ontology\n",
    "    tfidf = TfidfVectorizer(ngram_range=(1, top_ngram), min_df=min_df)\n",
    "    tf_idf_united = tfidf.fit_transform(united_columns)\n",
    "    docs_words = pd.DataFrame(tf_idf_united.toarray(), columns=tfidf.get_feature_names_out())\n",
    "    \n",
    "    cooccured = {}\n",
    "\n",
    "    for cur_word in enumerate(docs_words.columns):\n",
    "        current_list = []\n",
    "        for other_word in enumerate(docs_words.columns):\n",
    "            if cur_word[0] != other_word[0] and np.count_nonzero(\n",
    "                    pd.Series(docs_words.iloc[:, cur_word[0]] * docs_words.iloc[:, other_word[0]])) >= min_cooccur:\n",
    "                current_list.append(other_word[1])\n",
    "        cooccured[cur_word[1]] = current_list\n",
    "\n",
    "    work_nodes = set()\n",
    "    work_edges = []\n",
    "\n",
    "    for word in docs_words.columns:\n",
    "        for other_word in cooccured[word]:\n",
    "            if L(word, other_word, docs_words, cooccured) > 0.2:\n",
    "                work_nodes.add(word)\n",
    "                work_nodes.add(other_word)\n",
    "                work_edges.append(\n",
    "                    [other_word, word, {'weight': np.round(L(word, other_word, docs_words, cooccured), 4)}])\n",
    "\n",
    "    just_edges = {}\n",
    "\n",
    "    for edge in work_edges:\n",
    "        if edge[0] not in just_edges:\n",
    "            just_edges[edge[0]] = [edge[1]]\n",
    "        else:\n",
    "            just_edges[edge[0]].append(edge[1])\n",
    "\n",
    "    edge_for_removal = []\n",
    "\n",
    "    for cur_node in just_edges:\n",
    "        for other_node in just_edges[cur_node]:\n",
    "            if other_node in just_edges and set(just_edges[cur_node]) & set(just_edges[other_node]) != set():\n",
    "                edge_for_removal.append(\n",
    "                    [cur_node, other_node, list(set(just_edges[cur_node]) & set(just_edges[other_node]))])\n",
    "\n",
    "    for edges in edge_for_removal:\n",
    "        for member in edges[2]:\n",
    "            ind1 = find_ind_for_removal([edges[0], member], work_edges)\n",
    "            ind2 = find_ind_for_removal([edges[1], member], work_edges)\n",
    "\n",
    "            if ind1 is not None and ind2 is not None:\n",
    "                if work_edges[ind1][2]['weight'] >= work_edges[ind2][2]['weight']:\n",
    "                    work_edges.pop(ind2)\n",
    "                else:\n",
    "                    work_edges.pop(ind1)\n",
    "    \n",
    "    #Inputing the ontology into a graph structure\n",
    "    G1 = nx.DiGraph()\n",
    "    \n",
    "    if check_vis_list:\n",
    "        final_list = {}\n",
    "\n",
    "        for work_edge in work_edges:\n",
    "            if work_edge[0] not in final_list:\n",
    "                final_list[work_edge[0]] = []\n",
    "        \n",
    "            final_list[work_edge[0]].append([work_edge[1], work_edge[2]])\n",
    "\n",
    "        return final_list\n",
    "    \n",
    "    G1.add_nodes_from(work_nodes)\n",
    "    G1.add_edges_from(work_edges)\n",
    "    \n",
    "    #Setting up the parameters of visualisation\n",
    "    plt.rcParams[\"figure.figsize\"] = (20, 20)\n",
    "\n",
    "    pos = nx.spring_layout(G1, seed=47)\n",
    "    node_sizes = [i for i in range(len(G1))]\n",
    "\n",
    "    nodes = nx.draw_networkx_nodes(G1, pos, node_size=node_sizes, node_color=\"indigo\", alpha=0.75)\n",
    "\n",
    "    M = G1.number_of_edges()\n",
    "    edge_colors = range(2, M + 2)\n",
    "    edge_alphas = [(5 + i) / (M + 4) for i in range(M)]\n",
    "    cmap = plt.cm.plasma\n",
    "\n",
    "    edges = nx.draw_networkx_edges(\n",
    "            G1,\n",
    "            pos,\n",
    "            node_size=node_sizes,\n",
    "            arrowstyle=\"->\",\n",
    "            arrowsize=10,\n",
    "            edge_color=edge_colors,\n",
    "            edge_cmap=cmap,\n",
    "            width=1,\n",
    "        )\n",
    "\n",
    "    for i in range(M):\n",
    "        edges[i].set_alpha(edge_alphas[i])\n",
    "    \n",
    "    #Show the visualisation\n",
    "    nx.draw_networkx_labels(G1, pos, font_size=14, verticalalignment='top')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08f7a8e",
   "metadata": {},
   "source": [
    "Function to save it into csv (to be later uploaded into Neo4j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "01804e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_to_csv(vocab, node_file, edge_file):\n",
    "    \n",
    "    all_edges = []\n",
    "    all_nodes = set()\n",
    "    \n",
    "    for word in vocab.keys():\n",
    "        for i in vocab[word]:\n",
    "            all_edges.append([i[0], word, i[1]['weight']])\n",
    "            all_nodes.add(i[0])\n",
    "        all_nodes.add(word)\n",
    "    \n",
    "    nodes = pd.DataFrame(all_nodes, columns = ['Word'])\n",
    "    edges = pd.DataFrame(all_edges, columns = ['Word', 'Subtopic_of', 'Weight'])\n",
    "    \n",
    "    nodes.to_csv(node_file, index = False)\n",
    "    edges.to_csv(edge_file, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840e6520",
   "metadata": {},
   "source": [
    "Loading libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "139e4df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\anutk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anutk\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import html\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10051ff4",
   "metadata": {},
   "source": [
    "Preprocessing for the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d832adfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_for_remove_short(x):\n",
    "    '''\n",
    "    Removes tokens which have the length of 1\n",
    "    '''\n",
    "    line = ''\n",
    "    for i in str(x).split():\n",
    "        if len(i) > 1:\n",
    "            line += i + ' '\n",
    "\n",
    "    return line.strip()\n",
    "    \n",
    "def check_for_remove_stopwords(x):\n",
    "    '''\n",
    "    Removes stopwords from the line\n",
    "    '''\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    line = ''\n",
    "\n",
    "    for i in str(x).split():\n",
    "        if i not in stop_words:\n",
    "            line += i + ' '\n",
    "\n",
    "    return line.strip()\n",
    "\n",
    "class TextPreprocessor:\n",
    "    '''\n",
    "    Class for general text preprocessing of a dataset\n",
    "    Removes\n",
    "        - punctuation\n",
    "        - normalizes whitespaces after removal of punctuation\n",
    "        - tokens which have the length of 1\n",
    "        - stopwords\n",
    "        - numbers\n",
    "    Normalizes \n",
    "        - whitespaces after removal of punctuation\n",
    "    Lemmatizes all words in the dataset\n",
    "    Shortens the sentences if they are above word limit\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initilizes an object of the class\n",
    "        '''\n",
    "        self.data = None\n",
    "\n",
    "    def fit(self, data: pd.DataFrame) -> None:\n",
    "        '''\n",
    "        Fits the input data into an object of the class\n",
    "        '''\n",
    "        self.data = data\n",
    "\n",
    "    def fit_transform(self, data: pd.DataFrame, columns) -> pd.DataFrame:\n",
    "        '''\n",
    "        Fits and transforms the input data into an object of the class\n",
    "        '''\n",
    "        self.fit(data)\n",
    "        return self.transform(columns)\n",
    "\n",
    "    def transform(self, columns) -> pd.DataFrame:\n",
    "        '''\n",
    "        Tranforms the input data\n",
    "        Removes\n",
    "            - punctuation\n",
    "            - normalizes whitespaces after removal of punctuation\n",
    "            - tokens which have the length of 1\n",
    "            - stopwords\n",
    "            - numbers\n",
    "        Normalizes \n",
    "            - whitespaces after removal of punctuation\n",
    "        Lemmatizes all words in the dataset\n",
    "        Shortens the sentences if they are above word limit\n",
    "        '''\n",
    "        self.__remove_punc(columns)\n",
    "        self.__normalize_whitespace(columns)\n",
    "        self.__remove_short(columns)\n",
    "        self.__lemmatize(columns)\n",
    "        self.__remove_stopwords(columns)\n",
    "        self.__remove_numbers(columns)\n",
    "        return self.data\n",
    "\n",
    "    def __remove_punc(self, columns):\n",
    "        '''\n",
    "        Removes punctuation from the data\n",
    "        '''\n",
    "        pattern = re.compile(r'[^\\w\\s]+')\n",
    "        for column in columns:\n",
    "            self.data[column] = self.data[column].apply(lambda x: re.sub(pattern, ' ', str(x)))\n",
    "\n",
    "    def __normalize_whitespace(self, columns):\n",
    "        '''\n",
    "        Normalizes whitespaces\n",
    "        '''\n",
    "        for column in columns:\n",
    "            self.data[column] = self.data[column].str.replace('  ', ' ')\n",
    "\n",
    "    def __remove_short(self, columns):\n",
    "        '''\n",
    "        Removes short words which have the length of 1\n",
    "        '''\n",
    "        for column in columns:\n",
    "            self.data[column] = self.data[column].apply(check_for_remove_short)\n",
    "            \n",
    "    def __lemmatize(self, columns):\n",
    "        '''\n",
    "        Lemmatizes the words in the dataset\n",
    "        '''\n",
    "        morph = WordNetLemmatizer()\n",
    "        \n",
    "        for column in columns:\n",
    "            self.data[column] = self.data[column].apply(lambda x: ' '.join([morph.lemmatize(i.lower()) for i in x.split()]))\n",
    "\n",
    "    def __remove_stopwords(self, columns):\n",
    "        '''\n",
    "        Removes stopwords\n",
    "        '''\n",
    "        for column in columns:\n",
    "            self.data[column] = self.data[column].apply(check_for_remove_stopwords)\n",
    "        \n",
    "    def __shorten(self, columns):\n",
    "        '''\n",
    "        Shortens the lines which are above the word limit\n",
    "        '''\n",
    "        for column in columns:\n",
    "            self.data[column] = self.data[column].apply(lambda x: check_len(x, 256))\n",
    "    \n",
    "    def __remove_numbers(self, columns):\n",
    "        '''\n",
    "        Removes numbers\n",
    "        '''\n",
    "        for column in columns:\n",
    "            self.data[column] = self.data[column].apply(lambda x: re.sub('(\\d)', '', x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5994252f",
   "metadata": {},
   "source": [
    "Preprocessing and saving the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "69559037",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prep_df = pd.read_csv('bbc_data.csv')\n",
    "#TP = TextPreprocessor()\n",
    "#prep_df = TP.fit_transform(prep_df, ['data'])\n",
    "#prep_df.to_csv('bbc_processed_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "90257fba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       musician tackle u red tape musician group tack...\n",
       "1       us desire number one u three prestigious gramm...\n",
       "2       rocker doherty stage fight rock singer pete do...\n",
       "3       snicket top u box office chart film adaptation...\n",
       "4       ocean twelve raid box office ocean twelve crim...\n",
       "                              ...                        \n",
       "2220    warning window word file writing microsoft wor...\n",
       "2221    fast lift rise record book two high speed lift...\n",
       "2222    nintendo add medium playing nintendo releasing...\n",
       "2223    fast moving phone virus appear security firm w...\n",
       "2224    hacker threat apple itunes user apple music ju...\n",
       "Name: data, Length: 2225, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('bbc_processed_data.csv')\n",
    "df['data']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cffcb25",
   "metadata": {},
   "source": [
    "Creating the ontology via Klink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4d20b092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       word  cluster\n",
      "0  democrat        0\n",
      "1       lib        1\n",
      "2      dems        1\n",
      "3   liberal        0\n",
      "        word  cluster\n",
      "0       eral        0\n",
      "1  democrat_        0\n",
      "Empty DataFrame\n",
      "Columns: [word, cluster]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "created = create_ontology(df['data'], 0.015, 1)\n",
    "written = visualise_ontology(created, True, 0.015, 1)\n",
    "    \n",
    "node_name = 'bbc' + '_nodes.csv'\n",
    "edge_name = 'bbc' + '_edges.csv'\n",
    "\n",
    "graph_to_csv(written, node_name, edge_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
